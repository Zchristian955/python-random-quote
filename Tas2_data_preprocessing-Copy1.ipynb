{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-12cd98f76aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#importing the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m##essential columns exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check  the validity of an incoming data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#importing the dataset\n",
    "dataset = pd.read_csv()\n",
    "##essential columns exist\n",
    "\n",
    "if 'columns' in dataset.columns:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "\n",
    "\n",
    "#Data processing \n",
    "import numpy as np\n",
    "import pandas \n",
    "\n",
    "dataset = pd.read_csv()\n",
    "\n",
    "#X is defined as the matrix of independant variables \n",
    "X= dataset.iloc[:,'all columns'].values\n",
    "Y= dataset.iloc[:,'Column'].values\n",
    "\n",
    "\n",
    "# Taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:, 'columns'])\n",
    "# it used  on the ase that  columns are quantitative and there are a missing data\n",
    "X[:, 'columns'] = imputer.transform(X[:, 'columns'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Encoding the X Variable with 3 modalities\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 'column'] = labelencoder_X.fit_transform(X[:, 'column'])\n",
    "onehotencoder = OneHotEncoder(categorical_features = ['index of the column'])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# I add toarray() afer transform because my variable  have \n",
    "# 3 modality   i the cas we have only 2 modality ,  the exemple below of y encoded showed that\n",
    "\n",
    "\n",
    "\n",
    "# Encoding the categorical   Variable with two modalities\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =, random_state = 0)\n",
    "#test_size is used to specify the  length of X_test and y_test as new dataset and random_state= to get the same  annswer  as the first\n",
    "# sampling\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import  matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "#Plotting qualitative variable \n",
    "counts = dataset[\"column \"].value_counts()\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Boxplot\n",
    "import seaborn as sns \n",
    "sns.boxplot(dataset['columns']) \n",
    "sns.despine()\n",
    "\n",
    "\n",
    "\n",
    "#plot two quantitatvive variable ( Scatter Plot)\n",
    "sns.relplot(x = 'column1', y = 'column2',data = dataframe)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "## Let create your classifier and fit classifier on tranning set \n",
    "from  sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=3, metric = 'minkowski',p=2)\n",
    "#it used   metric = 'minkowski',p=2 to apply euclidean distance which is most used in KNN\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "#Predicting test result\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "#Visulaized the test set \n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('yellow', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('yellow', 'blue'))(i), label = j)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/fd70777afbda38b88d3eeb32e19c20b5"
  },
  "gist": {
   "data": {
    "description": "Task2_data_preprocessing",
    "public": true
   },
   "id": "fd70777afbda38b88d3eeb32e19c20b5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
